{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App\n",
    "\n",
    "> Translate an image based on a prompt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import base64\n",
    "import atexit\n",
    "import logging\n",
    "import threading\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from fastcore.basics import store_attr\n",
    "from safetensors.torch import load_file\n",
    "from diffusers import (\n",
    "    StableDiffusionXLControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel,\n",
    "    EulerDiscreteScheduler,\n",
    ")\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "from huggingface_hub import hf_hub_download\n",
    "from flask import Flask, jsonify, request, Response, render_template, stream_with_context\n",
    "from mmm_st.config import Config as BaseConfig\n",
    "from mmm_st.diffuse import BaseTransformer\n",
    "from mmm_st.video import convert_to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Create Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Basic logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Application configuration\n",
    "class Config:\n",
    "    HOST = '0.0.0.0'\n",
    "    PORT = 8989\n",
    "    CAP_PROPS = {}\n",
    "    NUM_STEPS = 4\n",
    "    HEIGHT = 1024\n",
    "    WIDTH = 1024\n",
    "    SEED  = BaseConfig.SEED\n",
    "    VIDEO_PATH = '/dev/video0'\n",
    "\n",
    "    # to display the final output frame on stream\n",
    "    DISPLAY_WIDTH = 1920\n",
    "    DISPLAY_HEIGHT = 1080\n",
    "\n",
    "    # alpha blending factor for frame interpolation\n",
    "    FRAME_BLEND = 0.75\n",
    "\n",
    "    # Models for image and controlnet\n",
    "    MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    UNET_ID = \"ByteDance/SDXL-Lightning\"\n",
    "    UNET_CKPT = \"sdxl_lightning_4step_unet.safetensors\"\n",
    "    CONTROLNET = \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "\n",
    "    # Default data type for torch\n",
    "    DTYPE = torch.float16\n",
    "\n",
    "# Ensure torch uses the correct seed\n",
    "torch.manual_seed(Config.SEED)\n",
    "\n",
    "\n",
    "# Class for managing shared resources\n",
    "class SharedResources:\n",
    "    def __init__(self):\n",
    "        self.image_transformer = SDXL(\n",
    "            num_steps=Config.NUM_STEPS,\n",
    "            height=Config.HEIGHT,\n",
    "            width=Config.WIDTH,\n",
    "        )\n",
    "        self.current_frame = None\n",
    "        self.current_prompt = None\n",
    "        self.lock = threading.Lock()  # Ensure thread-safe access\n",
    "        self.stop_event = threading.Event()  # Signal to stop thread\n",
    "\n",
    "    def update_frame(self, frame):\n",
    "        with self.lock:\n",
    "            self.current_frame = frame\n",
    "\n",
    "    def get_frame(self):\n",
    "        with self.lock:\n",
    "            return self.current_frame\n",
    "\n",
    "    def update_prompt(self, prompt):\n",
    "        with self.lock:\n",
    "            self.current_prompt = prompt\n",
    "\n",
    "    def get_prompt(self):\n",
    "        with self.lock:\n",
    "            return self.current_prompt\n",
    "\n",
    "\n",
    "class VideoStreamer:\n",
    "    \"Continuously reads frames from a video capture source.\"\n",
    "    def __init__(self, device_path='/dev/video0'):\n",
    "        self.cap = cv2.VideoCapture(device_path)\n",
    "        if not self.cap.isOpened():\n",
    "            logger.error(\"Failed to open video source\")\n",
    "            raise ValueError(\"Video source cannot be opened\")\n",
    "        for prop, value in Config.CAP_PROPS.items():\n",
    "            self.cap.set(getattr(cv2, prop), value)\n",
    "\n",
    "    def get_current_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return None\n",
    "        return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    def release(self):\n",
    "        self.cap.release()\n",
    "\n",
    "\n",
    "# Video processing class that handles the transformation and streaming\n",
    "class VideoProcessingThread(threading.Thread):\n",
    "    def __init__(self, shared_resources, device_path=Config.VIDEO_PATH):\n",
    "        super().__init__(daemon=True)\n",
    "        self.shared_resources = shared_resources\n",
    "        self.video_streamer = VideoStreamer(device_path)\n",
    "        self.previous_frame = None\n",
    "        self.stop_event = shared_resources.stop_event\n",
    "\n",
    "    def run(self):\n",
    "        def interpolate_images(image1, image2, alpha=0.5):\n",
    "            \"\"\" Interpolates two images with a given alpha blending factor. \"\"\"\n",
    "            if image1.size != image2.size:\n",
    "                image2 = image2.resize(image1.size)\n",
    "            return Image.blend(image1, image2, alpha)\n",
    "\n",
    "        while not self.stop_event.is_set():\n",
    "            current_frame = self.video_streamer.get_current_frame()\n",
    "            if current_frame is None:\n",
    "                continue\n",
    "\n",
    "            current_prompt = self.shared_resources.get_prompt()\n",
    "\n",
    "            # Apply transformation based on the current prompt\n",
    "            transformed_frame = self.shared_resources.image_transformer.transform(\n",
    "                current_frame,\n",
    "                current_prompt,\n",
    "            )\n",
    "\n",
    "            # Interpolate and update the shared frame\n",
    "            if self.previous_frame is not None:\n",
    "                final_frame = interpolate_images(self.previous_frame, transformed_frame, Config.FRAME_BLEND)\n",
    "            else:\n",
    "                final_frame = transformed_frame\n",
    "\n",
    "            self.shared_resources.update_frame(final_frame)\n",
    "            self.previous_frame = final_frame\n",
    "\n",
    "            # final sanity break\n",
    "            if self.stop_event.is_set(): break\n",
    "\n",
    "        self.video_streamer.release()  # Release resources when stopping\n",
    "\n",
    "\n",
    "class SDXL(BaseTransformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cfg=1.0,\n",
    "            strength=0.75,\n",
    "            canny_low_threshold=100,\n",
    "            canny_high_threshold=200,\n",
    "            controlnet_scale=0.5,\n",
    "            controlnet_start=0.0,\n",
    "            controlnet_end=1.0,\n",
    "            width=Config.WIDTH,\n",
    "            height=Config.HEIGHT,\n",
    "            dtype=Config.DTYPE,\n",
    "            **kwargs,\n",
    "        ):\n",
    "        store_attr()\n",
    "        self.generator = torch.Generator(device=\"cpu\").manual_seed(Config.SEED)\n",
    "        self.device = torch.device(self.device)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _initialize_pipeline(self):\n",
    "\n",
    "        # initialize the control net model\n",
    "        controlnet = ControlNetModel.from_pretrained(\n",
    "            Config.CONTROLNET,\n",
    "            torch_dtype=self.dtype,\n",
    "            use_safetensors=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # for depth estimation\n",
    "        self.depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(self.device)\n",
    "        self.feature_extractor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "        # load in the VAE\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"madebyollin/sdxl-vae-fp16-fix\",\n",
    "            torch_dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        # Load the Lightning UNet\n",
    "        unet_config = UNet2DConditionModel.load_config(Config.MODEL_ID, subfolder=\"unet\")\n",
    "        unet = UNet2DConditionModel.from_config(unet_config).to(self.device, self.dtype)\n",
    "        unet.load_state_dict(load_file(hf_hub_download(Config.UNET_ID, Config.UNET_CKPT), device=self.device))\n",
    "        \n",
    "        # Load the model with a controlnet.\n",
    "        self.pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "            Config.MODEL_ID,\n",
    "            use_safetensors=True,\n",
    "            unet=unet,\n",
    "            vae=vae,\n",
    "            # NOTE: try using the same contorlnet twice, with different strengths and images\n",
    "            controlnet=controlnet,\n",
    "        )\n",
    "\n",
    "        # Ensure sampler uses \"trailing\" timesteps.\n",
    "        self.pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
    "            self.pipe.scheduler.config,\n",
    "            timestep_spacing=\"trailing\",\n",
    "        )\n",
    "\n",
    "        # final pipeline setup\n",
    "        self.pipe.set_progress_bar_config(disable=True)\n",
    "        self.pipe.to(device=self.device, dtype=self.dtype).to(self.device)\n",
    "        if self.device not in (\"mps\", torch.device(\"mps\")):\n",
    "            self.pipe.unet.to(memory_format=torch.channels_last)\n",
    "\n",
    "        # start with a fixed set of latents\n",
    "        self.latents = self._make_latents()\n",
    "\n",
    "\n",
    "    def _make_latents(self):\n",
    "        \"Create a fixed set of latents for reproducible starting images.\"\n",
    "        batch_size, num_images_per_prompt = 1, 1\n",
    "        batch_size *= num_images_per_prompt\n",
    "        num_channels_latents = self.pipe.unet.config.in_channels\n",
    "        shape = (batch_size, num_channels_latents, self.height // self.pipe.vae_scale_factor, self.width // self.pipe.vae_scale_factor)\n",
    "        latents = randn_tensor(shape, generator=self.generator, device=self.device, dtype=self.dtype)\n",
    "        return latents\n",
    "    \n",
    "    def refresh_latents(self):\n",
    "        \"Grabs a new set of latents to refresh the starting image generation.\"\n",
    "        self.latents = self._make_latents()\n",
    "\n",
    "    def get_latents(self):\n",
    "        \"Returns a copy of the current latents, to prevent them from being overriden.\"\n",
    "        return self.latents.clone()\n",
    "\n",
    "    def transform(self, image, prompt) -> Image.Image:\n",
    "        \"\"\"Transforms the given `image` based on the `prompt`.\n",
    "        \n",
    "        Uses a controlnet to condition and manage the generation. \n",
    "        \"\"\"\n",
    "\n",
    "        # get the controlnet image\n",
    "        control_image = self.get_depth_map(image)\n",
    "        steps = self.num_steps\n",
    "        if int(steps * self.strength) < 1:\n",
    "            steps = math.ceil(1 / max(0.10, self.strength))\n",
    "\n",
    "        # generate and return the image\n",
    "        results = self.pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=self.negative_prompt,\n",
    "            image=control_image,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=self.cfg,\n",
    "            strength=self.strength,\n",
    "            controlnet_conditioning_scale=self.controlnet_scale,\n",
    "            control_guidance_start=self.controlnet_start,\n",
    "            control_guidance_end=self.controlnet_end,\n",
    "            width=self.width,\n",
    "            height=self.height,\n",
    "            output_type=\"pil\",\n",
    "            generator=self.generator,\n",
    "            # add the fixed, starting latents\n",
    "            latents=self.get_latents(),\n",
    "        )\n",
    "        result_image = results.images[0]\n",
    "        return result_image\n",
    "    \n",
    "    def get_depth_map(self, image):\n",
    "        image = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "        with torch.no_grad(), torch.autocast(self.device):\n",
    "            depth_map = self.depth_estimator(image).predicted_depth\n",
    "\n",
    "        depth_map = torch.nn.functional.interpolate(\n",
    "            depth_map.unsqueeze(1),\n",
    "            size=(self.height, self.width),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "        depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "        depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "        image = torch.cat([depth_map] * 3, dim=1)\n",
    "\n",
    "        image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "        image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
    "        return image\n",
    "    \n",
    "\n",
    "# Initialize shared resources and video processing thread\n",
    "shared_resources = SharedResources()\n",
    "video_thread = VideoProcessingThread(shared_resources)\n",
    "video_thread.start()\n",
    "\n",
    "\n",
    "# Flask endpoint to serve the index page\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "# Flask endpoint to set the prompt\n",
    "@app.route('/set_prompt', methods=['POST'])\n",
    "def set_prompt():\n",
    "    prompt = request.json.get('prompt')\n",
    "    if not prompt:\n",
    "        return jsonify({\"error\": \"Prompt is required\"}), 400\n",
    "    shared_resources.update_prompt(prompt)  # Update the prompt\n",
    "    return jsonify({\"message\": \"Prompt set successfully\"})\n",
    "\n",
    "\n",
    "# Flask endpoint to refresh the latents\n",
    "@app.route('/refresh_latents', methods=['POST'])\n",
    "def refresh_latents():\n",
    "    try:\n",
    "        signal = request.json.get('signal')\n",
    "        if signal != \"refresh\":\n",
    "            return jsonify({\"error\": \"Invalid signal\"}), 400\n",
    "        \n",
    "        # Refresh the latents\n",
    "        with shared_resources.lock:\n",
    "            shared_resources.image_transformer.refresh_latents()\n",
    "        \n",
    "        return jsonify({\"message\": \"Latents refreshed\"})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error refreshing latents: {e}\")\n",
    "        return jsonify({\"error\": \"Could not refresh latents\"}), 500\n",
    "\n",
    "\n",
    "# Flask endpoint to stream the latest frame\n",
    "@app.route('/stream')\n",
    "def stream():\n",
    "    def generate():\n",
    "        while not shared_resources.stop_event.is_set():\n",
    "            frame = shared_resources.get_frame()\n",
    "            if frame is not None:\n",
    "                frame = convert_to_pil_image(frame)\n",
    "                # rescale to better fit the image\n",
    "                frame = frame.resize((Config.DISPLAY_WIDTH, Config.DISPLAY_HEIGHT))\n",
    "                img_byte_arr = BytesIO()\n",
    "                frame.save(img_byte_arr, format='JPEG')\n",
    "                img_byte_arr.seek(0)\n",
    "                encoded_img = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
    "                yield f\"data: {encoded_img}\\n\\n\"\n",
    "                # time.sleep(1 / 30)  # Frame rate control\n",
    "\n",
    "    return Response(stream_with_context(generate()), mimetype='text/event-stream')\n",
    "\n",
    "\n",
    "# Cleanup function for releasing resources\n",
    "def cleanup():\n",
    "    shared_resources.stop_event.set() \n",
    "    video_thread.join() \n",
    "    gc.collect()  \n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "# Register cleanup function at exit\n",
    "atexit.register(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(\n",
    "        host=Config.HOST, \n",
    "        port=Config.PORT,\n",
    "        debug=True, \n",
    "        threaded=True, \n",
    "        use_reloader=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
