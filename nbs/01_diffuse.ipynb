{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffuse\n",
    "\n",
    "> Transforms an image based on a given prompt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp diffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from fastcore.basics import store_attr\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "torch.set_default_device(\"mps\")\n",
    "from diffusers.utils import load_image\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from mmm_st.config import Config\n",
    "from PIL import Image\n",
    "from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseTransformer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name=Config.MODEL_NAME,\n",
    "            device=Config.DEVICE,\n",
    "            img_size=Config.IMG_SIZE,\n",
    "            num_steps=Config.NUM_STEPS,\n",
    "        ):\n",
    "        store_attr()\n",
    "        self._initialize_pipeline()\n",
    "\n",
    "    def _initialize_pipeline(self):\n",
    "        raise NotImplementedError(\"Subclasses should implement this method.\")\n",
    "\n",
    "    def transform_image(self, image, prompt):\n",
    "        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\n",
    "    def prepare_image(self, image):\n",
    "        \"\"\" Prepare the image for transformation, converting it to the correct size and format. \"\"\"\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "        return image.resize(self.img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ImageTransformer(BaseTransformer):\n",
    "    \"\"\"\n",
    "    A class to manage image transformation using the Hugging Face diffusers pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        store_attr()  # Store initialization parameters for easy access later\n",
    "        self._initialize_pipeline()\n",
    "\n",
    "    def _initialize_pipeline(self):\n",
    "        \"\"\"\n",
    "        Initializes the img2img pipeline with model settings optimized for performance.\n",
    "        \"\"\"\n",
    "        pipe = AutoPipelineForImage2Image.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True\n",
    "        ).to(self.device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        self.pipeline = pipe\n",
    "        \n",
    "\n",
    "    def transform_image(self, image, prompt):\n",
    "        \"\"\"\n",
    "        Transforms an image based on a given prompt using a pre-trained model.\n",
    "\n",
    "        Args:\n",
    "            image: A numpy array of the image to transform.\n",
    "            prompt (str): The creative prompt to guide the image transformation.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: The transformed image.\n",
    "        \"\"\"\n",
    "        image = self.prepare_image(image)\n",
    "\n",
    "        return self.pipeline(\n",
    "            prompt=prompt, \n",
    "            image=image,\n",
    "        ).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EdgeImageTransformer(BaseTransformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args, \n",
    "            canny_control_model,\n",
    "            low_threshold=100,\n",
    "            high_threshold=200,\n",
    "            **kwargs,\n",
    "        ):\n",
    "        store_attr()\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _initialize_pipeline(self):\n",
    "        controlnet = ControlNetModel.from_pretrained(self.canny_control_model, torch_dtype=torch.float16)\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            self.model_name,\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(self.device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        self.pipeline = pipe\n",
    "\n",
    "    def transform_image(self, image, prompt):\n",
    "        edge_image = cv2.Canny(image, self.low_threshold, self.high_threshold)\n",
    "        edge_image = np.stack((edge_image, edge_image, edge_image), axis=-1)\n",
    "        edge_image = self.prepare_image(edge_image)\n",
    "        return self.pipeline(\n",
    "            prompt=prompt,\n",
    "            images=[edge_image],\n",
    "            num_inference_steps=self.num_steps,\n",
    "        ).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PoseImageTransformer(BaseTransformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            pose_control_model=Config.CONTROL_NET_CANNY_MODEL,\n",
    "            pose_det_model=Config.POSE_DET_MODEL,\n",
    "            **kwargs,\n",
    "        ):\n",
    "        store_attr()\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _initialize_pipeline(self):\n",
    "        self.pose_model = OpenposeDetector.from_pretrained(self.pose_det_model)\n",
    "        controlnet = ControlNetModel.from_pretrained(self.pose_control_model, torch_dtype=torch.float16)\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            self.model_name,\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(self.device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        self.pipeline = pipe\n",
    "\n",
    "    def transform_image(self, image, prompt):\n",
    "        pose_image = self.pose_model(image)\n",
    "        pose_image = self.prepare_image(pose_image)\n",
    "        return self.pipeline(\n",
    "            prompt=prompt, \n",
    "            images=[pose_image], \n",
    "            num_inference_steps=self.num_steps,\n",
    "        ).images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CombinedImageTransformer(BaseTransformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args, \n",
    "            canny_control_model,\n",
    "            pose_control_model,\n",
    "            pose_det_model,\n",
    "            low_threshold=100,\n",
    "            high_threshold=200,\n",
    "            **kwargs,\n",
    "        ):\n",
    "        store_attr()\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _initialize_pipeline(self):\n",
    "        self.pose_model = OpenposeDetector.from_pretrained(self.pose_det_model)\n",
    "        controlnet = [\n",
    "            ControlNetModel.from_pretrained(self.pose_control_model, torch_dtype=torch.float16),\n",
    "            ControlNetModel.from_pretrained(self.canny_control_model, torch_dtype=torch.float16),\n",
    "        ]\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            self.model_name,\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(self.device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        self.pipeline = pipe\n",
    "\n",
    "    def transform_image(self, image, prompt):\n",
    "        prepared_pose_image = self.prepare_image(self.pose_model(image))\n",
    "        edge_image = cv2.Canny(image, self.low_threshold, self.high_threshold)\n",
    "        edge_image = np.stack((edge_image, edge_image, edge_image), axis=-1)\n",
    "        prepared_canny_image = self.prepare_image(edge_image)\n",
    "        return self.pipeline(\n",
    "            prompt=prompt,\n",
    "            images=[prepared_pose_image, prepared_canny_image],\n",
    "            negative_prompt=[Config.NEGATIVE_PROMPT],\n",
    "            num_inference_steps=self.num_steps,\n",
    "            controlnet_conditioning_scale=[1.0, 0.8]\n",
    "        ).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class KandinskyTransformer(BaseTransformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args, \n",
    "            model_name=Config.MODEL_NAME,\n",
    "            device=Config.DEVICE,\n",
    "            img_size=Config.IMG_SIZE,\n",
    "            num_steps=Config.NUM_STEPS,\n",
    "            **kwargs,\n",
    "        ):\n",
    "        store_attr()\n",
    "        super().__init__(*args, model_name=model_name, device=device, img_size=img_size, num_steps=num_steps, **kwargs)\n",
    "        self._initialize_pipeline()\n",
    "\n",
    "    def _initialize_pipeline(self):\n",
    "        self.depth_estimator = pipeline(\"depth-estimation\", device=self.device)\n",
    "        self.prior_pipeline = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-prior\",\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.pipeline = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-controlnet-depth\",\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True,\n",
    "        ).to(self.device)\n",
    "        self.pipeline.enable_model_cpu_offload()\n",
    "\n",
    "    def make_hint(self, image):\n",
    "        image = self.depth_estimator(image)[\"depth\"]\n",
    "        image = np.array(image)\n",
    "        image = image[:, :, None]\n",
    "        image = np.concatenate([image, image, image], axis=-1)\n",
    "        detected_map = torch.from_numpy(image).float() / 255.0\n",
    "        hint = detected_map.permute(2, 0, 1)\n",
    "        return hint\n",
    "\n",
    "    def transform_image(self, image, prompt, negative_prompt, generator):\n",
    "        img = self.prepare_image(image)\n",
    "        hint = self.make_hint(img).unsqueeze(0).half().to(self.device)\n",
    "        \n",
    "        img_emb = self.prior_pipeline(prompt=prompt, image=img, strength=0.85, generator=generator)\n",
    "        negative_emb = self.prior_pipeline(prompt=negative_prompt, image=img, strength=1, generator=generator)\n",
    "        \n",
    "        result_image = self.pipeline(\n",
    "            image=img, \n",
    "            strength=0.7, \n",
    "            image_embeds=img_emb.image_embeds, \n",
    "            negative_image_embeds=negative_emb.image_embeds, \n",
    "            hint=hint, \n",
    "            num_inference_steps=self.num_steps, \n",
    "            generator=generator, \n",
    "            height=self.img_size[0], \n",
    "            width=self.img_size[1],\n",
    "        ).images[0]\n",
    "        return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_transformer(transformer_type):\n",
    "    if transformer_type == \"regular\":\n",
    "        return ImageTransformer()\n",
    "    elif transformer_type == \"canny\":\n",
    "        return EdgeImageTransformer()\n",
    "    elif transformer_type == \"pose\":\n",
    "        return PoseImageTransformer()\n",
    "    elif transformer_type == \"combined\":\n",
    "        return CombinedImageTransformer()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown transformer type provided\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, CLIPTextModel, CLIPVisionModel\n",
    "from diffusers.utils import load_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f820c2a38c8641789c001d43d61da451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
