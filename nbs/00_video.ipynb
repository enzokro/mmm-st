{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video\n",
    "\n",
    "> Class to process frames from a video file or camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import logging\n",
    "import torch\n",
    "from PIL import ImageChops, Image\n",
    "from typing import Optional, Union, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def interpolate_images(image1, image2, alpha):\n",
    "    \"\"\"\n",
    "    Interpolates two images with a given alpha blending factor.\n",
    "    Args:\n",
    "        image1: Input image which can be a NumPy array, PyTorch tensor, or PIL image.\n",
    "        image2: Input image which can be a NumPy array, PyTorch tensor, or PIL image.\n",
    "        alpha: Alpha blending factor, a float between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        A PIL Image object representing the blended image.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch tensors to PIL images\n",
    "    if torch.is_tensor(image1):\n",
    "        # Ensure tensor is on CPU and in byte format\n",
    "        if image1.device != torch.device('cpu'):\n",
    "            image1 = image1.to('cpu')\n",
    "        image1 = image1.numpy() if image1.requires_grad else image1.detach().numpy()\n",
    "        image1 = Image.fromarray(np.uint8(image1))\n",
    "\n",
    "    if torch.is_tensor(image2):\n",
    "        # Ensure tensor is on CPU and in byte format\n",
    "        if image2.device != torch.device('cpu'):\n",
    "            image2 = image2.to('cpu')\n",
    "        image2 = image2.numpy() if image2.requires_grad else image2.detach().numpy()\n",
    "        image2 = Image.fromarray(np.uint8(image2))\n",
    "\n",
    "    # Convert NumPy arrays to PIL images\n",
    "    if isinstance(image1, np.ndarray):\n",
    "        image1 = Image.fromarray(image1)\n",
    "\n",
    "    if isinstance(image2, np.ndarray):\n",
    "        image2 = Image.fromarray(image2)\n",
    "\n",
    "    image1 = image1.resize(image2.size)\n",
    "\n",
    "    # Perform the blending\n",
    "    return ImageChops.blend(image1, image2, alpha)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_pil_image(image):\n",
    "    \"\"\"\n",
    "    Converts an input image which can be a NumPy array, PyTorch tensor, or PIL image to a PIL Image.\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        return image\n",
    "    elif torch.is_tensor(image):\n",
    "        # Convert PyTorch tensor to PIL Image\n",
    "        if image.device != torch.device('cpu'):\n",
    "            image = image.to('cpu')\n",
    "        image = image.numpy() if image.requires_grad else image.detach().numpy()\n",
    "        if image.ndim == 3 and image.shape[0] in {1, 3}:\n",
    "            # Handle channels of tensor assuming CHW format\n",
    "            image = image.transpose(1, 2, 0)  # Convert to HWC\n",
    "        if image.shape[2] == 1:  # If single-channel image, convert to grayscale\n",
    "            image = image[:, :, 0]\n",
    "        return Image.fromarray((image * 255).astype(np.uint8) if image.dtype == np.float32 else image)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        # Convert NumPy array to PIL Image\n",
    "        if image.ndim == 3 and image.shape[2] == 1:  # If single-channel image\n",
    "            image = image[:, :, 0]\n",
    "        return Image.fromarray(image)\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported image type\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class VideoStreamer:\n",
    "    \"Continuously reads frames from a video capture source.\"\n",
    "    \n",
    "    class Break(Exception):\n",
    "        \"Custom exception to break out of the video capture loop.\"\n",
    "        pass\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            video_source: Union[str, int],\n",
    "            frame_buffer_size: int = 10,\n",
    "            capture_props: Optional[Dict[str, Union[int, float]]] = None,\n",
    "            event = None,\n",
    "        ):\n",
    "        \"Initializes a video stream with a frame buffer and optional capture properties.\"\n",
    "        \n",
    "        # camera and buffer settings\n",
    "        self.video_source = video_source\n",
    "        self.capture_props = capture_props or {}\n",
    "        self.cap = None\n",
    "\n",
    "        # buffer settings\n",
    "        self.frame_buffer_size = frame_buffer_size\n",
    "        self.current_frame = None\n",
    "        self.frame_buffer = []\n",
    "\n",
    "        # handles thread in the background\n",
    "        self.thread = None\n",
    "        self.stop_event = event or threading.Event()\n",
    "    \n",
    "    def start(self):\n",
    "        \"Start capturing video frames in a background thread.\"\n",
    "        self.cap = cv2.VideoCapture(self.video_source)\n",
    "        if not self.cap.isOpened():\n",
    "            logger.error(f\"Failed to open video source: {self.video_source}\")\n",
    "            raise ValueError(f\"Failed to open video source: {self.video_source}\")\n",
    "        # setup and start camera\n",
    "        self._set_capture_props()\n",
    "        self._start_frame_thread()\n",
    "    \n",
    "    def _set_capture_props(self):\n",
    "        \"Set the video capture properties based on the provided configuration.\"\n",
    "        for prop, value in self.capture_props.items():\n",
    "            self.cap.set(getattr(cv2, prop), value)\n",
    "        # view some info about the opened camera\n",
    "        fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
    "        logger.info(f\"Video source opened with FPS: {fps}\")\n",
    "    \n",
    "    def _start_frame_thread(self):\n",
    "        \"Start the background thread for reading frames.\"\n",
    "        self.thread = threading.Thread(target=self._read_frames, daemon=True)\n",
    "        self.thread.start()\n",
    "        logger.info(f\"Started video capture from source: {self.video_source}\")\n",
    "    \n",
    "    def _read_frames(self):\n",
    "        \"Reads frames until signalled to stop.\"\n",
    "        while not self.stop_event.is_set():\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                self.current_frame = frame\n",
    "                self._update_frame_buffer(frame)\n",
    "            else:\n",
    "                logger.warning(f\"Failed to read frame from video source: {self.video_source}\")\n",
    "                break\n",
    "            # small check for graceful shutdown\n",
    "            if self.stop_event.is_set():\n",
    "                break\n",
    "    \n",
    "    def _update_frame_buffer(self, frame):\n",
    "        \"Updates the frame buffer with the latest frame.\"\n",
    "        if len(self.frame_buffer) >= self.frame_buffer_size:\n",
    "            self.frame_buffer.pop(0)\n",
    "        self.frame_buffer.append(frame)\n",
    "    \n",
    "    def get_current_frame(self) -> Optional[np.ndarray]:\n",
    "        \"Get the current frame from the video capture.\"\n",
    "        return self.current_frame\n",
    "    \n",
    "    def get_frame_buffer(self) -> List[np.ndarray]:\n",
    "        \"Get the frame buffer containing the most recent frames.\"\n",
    "        return self.frame_buffer\n",
    "    \n",
    "    def stop(self):\n",
    "        \"Stop the video capture and release resources.\"\n",
    "        self.stop_event.set()\n",
    "        self._join_frame_thread()\n",
    "        self._release_capture()\n",
    "        logger.info(f\"Stopped video capture from source: {self.video_source}\")\n",
    "    \n",
    "    def _join_frame_thread(self):\n",
    "        \"Wait for the frame reading thread to finish.\"\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join()\n",
    "    \n",
    "    def _release_capture(self):\n",
    "        \"Release the video capture resources.\"\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"Context manager entry point.\"\n",
    "        self.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"Context manager exit point.\"\n",
    "        self.stop()\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"Destructor to ensure resources are released.\"\n",
    "        self.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
